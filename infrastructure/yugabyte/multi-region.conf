# YugabyteDB Multi-Region Configuration
# VoxGuard Production Setup
#
# Geo-Distributed Deployment:
# - Lagos (Primary): 3 nodes, preferred leader location
# - Abuja (Replica): 1 node, read replica
# - Asaba (Replica): 1 node, read replica
#
# Total: 5 nodes with RF=3 (Replication Factor)

# ============================================================================
# CLUSTER CONFIGURATION
# ============================================================================

# Cluster ID and Name
cluster_id: voxguard-prod
cluster_name: VoxGuard Production

# Replication Factor
# With 5 nodes across 3 regions, RF=3 provides strong consistency
# while tolerating 1 node failure
replication_factor: 3

# Placement Policy
# Define geographic distribution of data replicas
placement_info:
  num_replicas: 3
  placement_blocks:
    - cloud: "on-premise"
      region: "lagos"
      zone: "lagos-az1"
      min_num_replicas: 1
    - cloud: "on-premise"
      region: "lagos"
      zone: "lagos-az2"
      min_num_replicas: 1
    - cloud: "on-premise"
      region: "abuja"
      zone: "abuja-az1"
      min_num_replicas: 1

# Leader Preference
# Prefer Lagos for write operations to minimize latency
leader_preference:
  - region: "lagos"
    zone: "lagos-az1"
    priority: 1
  - region: "lagos"
    zone: "lagos-az2"
    priority: 2
  - region: "abuja"
    zone: "abuja-az1"
    priority: 3

# Read Replica Configuration
# Asaba serves as read-only replica for local reads
read_replicas:
  - region: "asaba"
    zone: "asaba-az1"
    num_replicas: 1

# ============================================================================
# KUBERNETES DEPLOYMENT - YUGABYTEDB OPERATOR
# ============================================================================

---
apiVersion: v1
kind: Namespace
metadata:
  name: yb-platform

---
apiVersion: yugabyte.com/v1alpha1
kind: YBCluster
metadata:
  name: voxguard-cluster
  namespace: yb-platform
spec:
  # Image configuration
  image:
    repository: yugabytedb/yugabyte
    tag: 2.18.1.0-b1
    pullPolicy: IfNotPresent

  # Replication factor
  replicationFactor: 3

  # Enable TLS encryption
  enableTLS: true
  tls:
    enabled: true
    nodeToNode: true
    clientToNode: true
    rootCA:
      cert: yugabyte-ca-cert
      key: yugabyte-ca-key

  # Master server configuration (3 masters for HA)
  master:
    replicas: 3

    # Resource allocation
    resources:
      requests:
        cpu: 2000m
        memory: 4Gi
      limits:
        cpu: 4000m
        memory: 8Gi

    # Storage configuration
    storage:
      size: 100Gi
      storageClass: ssd

    # Master server flags
    masterGFlags:
      # Placement configuration
      placement_cloud: "on-premise"
      placement_region: "lagos"
      placement_zone: "lagos-az1"

      # Replication settings
      replication_factor: "3"

      # Leader balancing
      enable_load_balancing: "true"
      leader_balance_threshold: "1.5"

      # Performance tuning
      yb_num_shards_per_tserver: "8"
      ysql_num_shards_per_tserver: "8"

      # Logging
      log_min_seconds_to_retain: "86400"  # 24 hours
      max_log_size: "256"  # 256 MB

      # Networking
      rpc_bind_addresses: "0.0.0.0"
      webserver_interface: "0.0.0.0"

    # Pod placement rules
    podManagementPolicy: Parallel
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - yb-master
          topologyKey: kubernetes.io/hostname

  # Tablet server configuration
  tserver:
    replicas: 5  # 3 Lagos + 1 Abuja + 1 Asaba

    # Resource allocation
    resources:
      requests:
        cpu: 4000m
        memory: 8Gi
      limits:
        cpu: 8000m
        memory: 16Gi

    # Storage configuration
    storage:
      size: 500Gi
      storageClass: ssd

    # Tablet server flags
    tserverGFlags:
      # Placement configuration (overridden per region)
      placement_cloud: "on-premise"
      placement_region: "lagos"
      placement_zone: "lagos-az1"

      # Memory configuration
      memory_limit_hard_bytes: "15032385536"  # 14 GB
      default_memory_limit_to_ram_ratio: "0.85"

      # Cache configuration
      db_block_cache_size_bytes: "4294967296"  # 4 GB
      db_write_buffer_size: "268435456"  # 256 MB

      # Compaction configuration
      rocksdb_level0_file_num_compaction_trigger: "5"
      rocksdb_max_background_compactions: "3"
      rocksdb_max_background_flushes: "3"

      # Replication
      replication_factor: "3"
      min_leader_stepdown_retry_interval_ms: "20000"

      # YSQL configuration
      ysql_enable_auth: "true"
      ysql_hba_conf_csv: "host all yugabyte 0.0.0.0/0 trust,host all all 0.0.0.0/0 md5"
      ysql_num_shards_per_tserver: "8"
      ysql_max_connections: "300"

      # YCQL configuration
      use_cassandra_authentication: "true"
      cql_proxy_bind_address: "0.0.0.0"

      # Performance
      yb_client_admin_operation_timeout_sec: "120"
      max_stale_read_bound_time_ms: "10000"

      # Logging
      log_min_seconds_to_retain: "86400"
      max_log_size: "256"

    # Pod placement rules
    podManagementPolicy: Parallel
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - yb-tserver
            topologyKey: kubernetes.io/hostname

  # Network configuration
  enableLoadBalancer: true

  # Services
  services:
    yb-master-ui:
      type: LoadBalancer
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-internal: "true"
    yb-tserver-service:
      type: ClusterIP

---
# Service for YSQL (PostgreSQL-compatible)
apiVersion: v1
kind: Service
metadata:
  name: yb-tserver-ysql
  namespace: yb-platform
  labels:
    app: yb-tserver
spec:
  type: LoadBalancer
  selector:
    app: yb-tserver
  ports:
  - name: ysql
    port: 5433
    targetPort: 5433
    protocol: TCP

---
# Service for YCQL (Cassandra-compatible)
apiVersion: v1
kind: Service
metadata:
  name: yb-tserver-ycql
  namespace: yb-platform
  labels:
    app: yb-tserver
spec:
  type: LoadBalancer
  selector:
    app: yb-tserver
  ports:
  - name: ycql
    port: 9042
    targetPort: 9042
    protocol: TCP

---
# Service for Redis-compatible API
apiVersion: v1
kind: Service
metadata:
  name: yb-tserver-yedis
  namespace: yb-platform
  labels:
    app: yb-tserver
spec:
  type: ClusterIP
  selector:
    app: yb-tserver
  ports:
  - name: yedis
    port: 6379
    targetPort: 6379
    protocol: TCP

# ============================================================================
# TABLESPACE CONFIGURATION FOR GEO-PARTITIONING
# ============================================================================

# Create tablespaces for each region
# This allows pinning specific tables to specific regions

-- Tablespace for Lagos (Primary writes)
CREATE TABLESPACE lagos_tablespace WITH (
  replica_placement='{"num_replicas":3, "placement_blocks":[
    {"cloud":"on-premise","region":"lagos","zone":"lagos-az1","min_num_replicas":1},
    {"cloud":"on-premise","region":"lagos","zone":"lagos-az2","min_num_replicas":1},
    {"cloud":"on-premise","region":"abuja","zone":"abuja-az1","min_num_replicas":1}
  ]}'
);

-- Tablespace for Abuja (Read replicas)
CREATE TABLESPACE abuja_tablespace WITH (
  replica_placement='{"num_replicas":2, "placement_blocks":[
    {"cloud":"on-premise","region":"abuja","zone":"abuja-az1","min_num_replicas":1},
    {"cloud":"on-premise","region":"lagos","zone":"lagos-az1","min_num_replicas":1}
  ]}'
);

-- Tablespace for Asaba (Read replicas)
CREATE TABLESPACE asaba_tablespace WITH (
  replica_placement='{"num_replicas":2, "placement_blocks":[
    {"cloud":"on-premise","region":"asaba","zone":"asaba-az1","min_num_replicas":1},
    {"cloud":"on-premise","region":"lagos","zone":"lagos-az1","min_num_replicas":1}
  ]}'
);

-- Create VoxGuard tables with geo-partitioning
CREATE TABLE fraud_alerts (
  alert_id UUID PRIMARY KEY,
  timestamp TIMESTAMP,
  severity VARCHAR(20),
  source_number VARCHAR(20),
  destination_number VARCHAR(20),
  gateway_id UUID,
  detection_reason TEXT,
  action_taken VARCHAR(50),
  region VARCHAR(20)
) TABLESPACE lagos_tablespace
PARTITION BY LIST (region);

-- Partition for Lagos data
CREATE TABLE fraud_alerts_lagos PARTITION OF fraud_alerts
  FOR VALUES IN ('lagos') TABLESPACE lagos_tablespace;

-- Partition for Abuja data
CREATE TABLE fraud_alerts_abuja PARTITION OF fraud_alerts
  FOR VALUES IN ('abuja') TABLESPACE abuja_tablespace;

-- Partition for Asaba data
CREATE TABLE fraud_alerts_asaba PARTITION OF fraud_alerts
  FOR VALUES IN ('asaba') TABLESPACE asaba_tablespace;

# ============================================================================
# READ REPLICA CONFIGURATION
# ============================================================================

# Add read replicas for Abuja and Asaba regions
# Read replicas provide low-latency reads for non-critical queries

# CLI command to add read replicas
yb-admin \
  -master_addresses lagos-master-1:7100,lagos-master-2:7100,lagos-master-3:7100 \
  add_read_replica_placement_info \
  on-premise.abuja.abuja-az1,on-premise.asaba.asaba-az1 \
  1

# Verify read replica status
yb-admin \
  -master_addresses lagos-master-1:7100,lagos-master-2:7100,lagos-master-3:7100 \
  get_universe_config

# ============================================================================
# MONITORING AND HEALTH CHECKS
# ============================================================================

# Check cluster health
yb-admin \
  -master_addresses lagos-master-1:7100,lagos-master-2:7100,lagos-master-3:7100 \
  list_all_tablet_servers

# Check replication lag
SELECT * FROM yb_replication_status();

# Check table placement
SELECT * FROM yb_table_properties('fraud_alerts');

# Monitor tablet distribution
SELECT * FROM yb_local_tablets;

# Check load balancing
yb-admin \
  -master_addresses lagos-master-1:7100,lagos-master-2:7100,lagos-master-3:7100 \
  get_load_balancer_state

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================

# Optimize for write-heavy workload (fraud detection)
ALTER TABLE fraud_alerts SET (
  tablet_split_size_threshold_mb = 512,
  tablet_split_low_phase_size_threshold_mb = 128,
  tablet_split_low_phase_shard_count_per_node = 8
);

# Create indexes for common queries
CREATE INDEX idx_alerts_timestamp ON fraud_alerts (timestamp DESC)
  INCLUDE (severity, source_number)
  TABLESPACE lagos_tablespace;

CREATE INDEX idx_alerts_gateway ON fraud_alerts (gateway_id)
  WHERE action_taken = 'BLOCKED'
  TABLESPACE lagos_tablespace;

# Enable query statistics
SET pg_stat_statements.track = all;
SET pg_stat_statements.max = 10000;

# ============================================================================
# BACKUP AND DISASTER RECOVERY
# ============================================================================

# Configure backup schedule
yb-admin \
  -master_addresses lagos-master-1:7100 \
  create_snapshot_schedule \
  ysql.voxguard \
  86400 \
  604800  # Daily snapshots, 7-day retention

# Manual backup
yb-admin \
  -master_addresses lagos-master-1:7100 \
  create_snapshot \
  ysql.voxguard

# List snapshots
yb-admin \
  -master_addresses lagos-master-1:7100 \
  list_snapshots

# Restore from snapshot
yb-admin \
  -master_addresses lagos-master-1:7100 \
  restore_snapshot \
  <snapshot_id>

# Export backup to S3
yb-admin \
  -master_addresses lagos-master-1:7100 \
  export_snapshot \
  <snapshot_id> \
  s3://voxguard-backups/yugabyte/

# ============================================================================
# FAILOVER PROCEDURES
# ============================================================================

# Manual leader election (if Lagos fails)
yb-admin \
  -master_addresses abuja-master-1:7100 \
  change_leader_election_preferred_zones \
  on-premise.abuja.abuja-az1

# Promote read replica to full replica (disaster recovery)
yb-admin \
  -master_addresses lagos-master-1:7100 \
  modify_placement_info \
  on-premise.lagos.lagos-az1,on-premise.abuja.abuja-az1,on-premise.asaba.asaba-az1 \
  3

# ============================================================================
# CONNECTION STRINGS
# ============================================================================

# YSQL (PostgreSQL-compatible)
# Primary (Lagos): postgresql://yugabyte@lagos-ysql-lb.voxguard.internal:5433/voxguard
# Replica (Abuja): postgresql://yugabyte@abuja-ysql-lb.voxguard.internal:5433/voxguard?target_session_attrs=read-only
# Replica (Asaba): postgresql://yugabyte@asaba-ysql-lb.voxguard.internal:5433/voxguard?target_session_attrs=read-only

# YCQL (Cassandra-compatible)
# Primary (Lagos): lagos-ycql-lb.voxguard.internal:9042
# Replica (Abuja): abuja-ycql-lb.voxguard.internal:9042
# Replica (Asaba): asaba-ycql-lb.voxguard.internal:9042

# ============================================================================
# SECURITY CONFIGURATION
# ============================================================================

# Enable authentication
--ysql_enable_auth=true
--use_cassandra_authentication=true

# Create admin user
CREATE ROLE voxguard_admin WITH LOGIN SUPERUSER PASSWORD 'CHANGEME';

# Create application user
CREATE ROLE voxguard_app WITH LOGIN PASSWORD 'CHANGEME';
GRANT CONNECT ON DATABASE voxguard TO voxguard_app;
GRANT USAGE ON SCHEMA public TO voxguard_app;
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO voxguard_app;

# Configure TLS
--certs_dir=/opt/yugabyte/certs
--allow_insecure_connections=false
--use_node_to_node_encryption=true
--use_client_to_server_encryption=true

# ============================================================================
# TROUBLESHOOTING
# ============================================================================

# High replication lag?
# 1. Check network latency between regions
# 2. Monitor tablet leader distribution
# 3. Increase replication threads: --num_concurrent_backfills_allowed=4
# 4. Check for under-replicated tablets

# Uneven load distribution?
# 1. Enable load balancing: yb-admin set_load_balancer_enabled 1
# 2. Check tablet counts per tserver
# 3. Manually move tablets if needed

# Connection timeouts?
# 1. Increase operation timeout: --yb_client_admin_operation_timeout_sec=120
# 2. Check network connectivity
# 3. Monitor master/tserver logs

# Out of memory?
# 1. Reduce block cache: --db_block_cache_size_bytes
# 2. Increase memory limit: --memory_limit_hard_bytes
# 3. Scale horizontally: Add more tservers
