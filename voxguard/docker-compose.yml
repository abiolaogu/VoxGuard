# ============================================================================
# VoxGuard - Voice Network Fraud Detection Platform
# Nigerian ICL Complete Stack Deployment
# Version: 2.0 | Date: 2026-01-30
# ============================================================================

services:
  # ==========================================================================
  # DETECTION ENGINE (Rust)
  # ==========================================================================
  acm-engine:
    build:
      context: ../../detection-engine
      dockerfile: Dockerfile
    image: acm/detection-engine:2.0
    container_name: acm-engine
    restart: unless-stopped
    ports:
      - "8080:8080"
      - "9090:9090"  # Prometheus metrics
    environment:
      RUST_LOG: info,acm_detection=debug
      RUST_BACKTRACE: 1
      
      # Server config
      ACM_HOST: 0.0.0.0
      ACM_PORT: 8080
      ACM_METRICS_PORT: 9090
      ACM_REGION: lagos
      ACM_NODE_ID: acm-engine-1
      
      # DragonflyDB
      DRAGONFLY_URL: redis://dragonfly:6379
      DRAGONFLY_POOL_SIZE: 32
      
      # YugabyteDB
      YUGABYTE_URL: postgres://opensips:${YUGABYTE_PASSWORD:-acm_secure_2026}@yugabyte:5433/opensips
      YUGABYTE_POOL_SIZE: 16
      
      # ClickHouse
      CLICKHOUSE_URL: http://clickhouse:8123
      CLICKHOUSE_DATABASE: acm
      
      # QuestDB (real-time analytics)
      QUESTDB_PG_HOST: questdb
      QUESTDB_PG_PORT: 8812
      QUESTDB_ILP_HOST: questdb
      QUESTDB_ILP_PORT: 9009
      QUESTDB_HTTP_HOST: questdb
      QUESTDB_HTTP_PORT: 9000
      
      # NCC Compliance
      NCC_ENABLED: ${NCC_ENABLED:-false}
      NCC_ATRS_URL: https://atrs-api.ncc.gov.ng/v1
      NCC_CLIENT_ID: ${NCC_CLIENT_ID:-}
      NCC_CLIENT_SECRET: ${NCC_CLIENT_SECRET:-}
      NCC_ICL_LICENSE: ${NCC_ICL_LICENSE:-}
      
      # Detection thresholds
      ACM_CPM_WARNING: 40
      ACM_CPM_CRITICAL: 60
      ACM_ACD_WARNING: 10
      ACM_ACD_CRITICAL: 5
    depends_on:
      dragonfly:
        condition: service_healthy
      yugabyte:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    networks:
      - acm-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G

  # ==========================================================================
  # OPENSIPS (SIP Server)
  # ==========================================================================
  opensips:
    image: opensips/opensips:3.4
    container_name: opensips
    restart: unless-stopped
    ports:
      - "5060:5060/udp"
      - "5060:5060/tcp"
      - "5061:5061/tcp"  # TLS
      - "8888:8888"      # MI HTTP
    volumes:
      - ../../opensips-integration/opensips-acm.cfg:/usr/local/etc/opensips/opensips.cfg:ro
      - opensips-tls:/etc/opensips/tls
    environment:
      # Database
      DB_HOST: yugabyte
      DB_PORT: 5433
      DB_USER: opensips
      DB_PASS: ${YUGABYTE_PASSWORD:-acm_secure_2026}
      DB_NAME: opensips
      
      # Cache
      REDIS_HOST: dragonfly
      REDIS_PORT: 6379
      
      # ACM Engine
      ACM_ENGINE_HOST: acm-engine
      ACM_ENGINE_PORT: 8080
    depends_on:
      acm-engine:
        condition: service_healthy
      dragonfly:
        condition: service_healthy
      yugabyte:
        condition: service_healthy
    networks:
      - acm-network
    cap_add:
      - NET_ADMIN
    ulimits:
      nofile:
        soft: 65536
        hard: 65536

  # ==========================================================================
  # DRAGONFLYDB (High-Performance Cache)
  # ==========================================================================
  dragonfly:
    image: docker.dragonflydb.io/dragonflydb/dragonfly:v1.14.0
    container_name: dragonfly
    restart: unless-stopped
    ports:
      - "6379:6379"
    command: >
      --maxmemory=4gb
      --proactor_threads=4
      --cache_mode=true
      --hz=100
      --tcp-keepalive=60
      --snapshot_cron="0 */6 * * *"
      --dir=/data
      --dbfilename=acm_cache.rdb
    volumes:
      - dragonfly-data:/data
    networks:
      - acm-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 5G
        reservations:
          cpus: '2'
          memory: 4G

  # ==========================================================================
  # YUGABYTEDB (Distributed PostgreSQL)
  # ==========================================================================
  yugabyte:
    image: yugabytedb/yugabyte:2.20.1.0-b97
    container_name: yugabyte
    restart: unless-stopped
    ports:
      - "5433:5433"   # YSQL
      - "9000:9000"   # Master web UI
      - "9042:9042"   # YCQL
      - "7100:7000"   # Master RPC (remapped from 7000 to avoid macOS AirPlay conflict)
    command: >
      bin/yugabyted start
      --base_dir=/home/yugabyte/yb_data
      --daemon=false
      --ui=true
      --tserver_flags="yb_enable_read_committed_isolation=true,ysql_enable_packed_row=true"
    environment:
      YSQL_USER: opensips
      YSQL_PASSWORD: ${YUGABYTE_PASSWORD:-acm_secure_2026}
      YSQL_DB: opensips
    volumes:
      - yugabyte-data:/home/yugabyte/yb_data
      - ../../database/yugabyte:/docker-entrypoint-initdb.d:ro
    networks:
      - acm-network
    healthcheck:
      test: ["CMD-SHELL", "pgrep -x yb-tserver > /dev/null && pgrep -x postgres > /dev/null"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 90s
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G

  # ==========================================================================
  # CLICKHOUSE (Analytics)
  # ==========================================================================
  clickhouse:
    image: clickhouse/clickhouse-server:24.1
    container_name: clickhouse
    restart: unless-stopped
    ports:
      - "8123:8123"   # HTTP
      - "9000:9001"   # Native (remapped to avoid YB conflict)
    volumes:
      - clickhouse-data:/var/lib/clickhouse
      - clickhouse-logs:/var/log/clickhouse-server
      - ../../database/clickhouse:/docker-entrypoint-initdb.d:ro
    environment:
      CLICKHOUSE_USER: acm
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-acm_analytics_2026}
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    networks:
      - acm-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G

  # ==========================================================================
  # QUESTDB (Real-time Time-Series Analytics - replaces kdb+)
  # ==========================================================================
  questdb:
    image: questdb/questdb:7.4.0
    container_name: questdb
    restart: unless-stopped
    ports:
      - "9009:9009"    # InfluxDB Line Protocol (high-speed ingestion)
      - "8812:8812"    # PostgreSQL wire protocol (queries)
      - "9003:9003"    # Min health server
      - "9000:9000"    # Web Console & REST API (remapped if needed)
    volumes:
      - questdb-data:/var/lib/questdb
    environment:
      QDB_CAIRO_COMMIT_LAG: 1000
      QDB_SHARED_WORKER_COUNT: 4
      QDB_PG_USER: admin
      QDB_PG_PASSWORD: quest
      QDB_TELEMETRY_ENABLED: "false"
      QDB_LINE_TCP_MAINTENANCE_JOB_INTERVAL: 1000
      # Performance tuning for high-throughput
      QDB_CAIRO_MAX_UNCOMMITTED_ROWS: 500000
      QDB_LINE_TCP_MSG_BUFFER_SIZE: 32768
      QDB_LINE_TCP_MAX_MEASUREMENT_SIZE: 8192
    networks:
      - acm-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9003"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G

  # ==========================================================================
  # HASURA GRAPHQL ENGINE
  # ==========================================================================
  hasura:
    image: hasura/graphql-engine:v2.36.0
    container_name: hasura
    restart: unless-stopped
    ports:
      - "8082:8080"
    environment:
      ## PostgreSQL database URL (YugabyteDB)
      HASURA_GRAPHQL_DATABASE_URL: postgres://opensips:${YUGABYTE_PASSWORD:-acm_secure_2026}@yugabyte:5433/opensips
      ## Enable the console served by server
      HASURA_GRAPHQL_ENABLE_CONSOLE: "true"
      ## Enable debugging mode
      HASURA_GRAPHQL_DEV_MODE: "true"
      ## Admin secret for console access
      HASURA_GRAPHQL_ADMIN_SECRET: ${HASURA_ADMIN_SECRET:-acm_hasura_secret}
      ## JWT authentication
      HASURA_GRAPHQL_JWT_SECRET: '{"type":"HS256", "key":"${JWT_SECRET:-change_me_in_production_32_chars!}"}'
      ## Unauthorized role
      HASURA_GRAPHQL_UNAUTHORIZED_ROLE: anonymous
      ## Enable WebSocket subscriptions
      HASURA_GRAPHQL_ENABLE_REMOTE_SCHEMA_PERMISSIONS: "true"
      ## CORS settings
      HASURA_GRAPHQL_CORS_DOMAIN: "*"
      ## Logging
      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log
      ## Metadata directory
      HASURA_GRAPHQL_METADATA_DIR: /hasura-metadata
    volumes:
      - ../../hasura/metadata:/hasura-metadata:ro
    depends_on:
      yugabyte:
        condition: service_healthy
    networks:
      - acm-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/healthz"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G

  # ==========================================================================
  # MANAGEMENT API (Go)
  # ==========================================================================
  management-api:
    build:
      context: ../../management-api
      dockerfile: Dockerfile
    image: acm/management-api:2.0
    container_name: management-api
    restart: unless-stopped
    ports:
      - "8081:8081"
    environment:
      PORT: 8081
      YUGABYTE_URL: postgres://opensips:${YUGABYTE_PASSWORD:-acm_secure_2026}@yugabyte:5433/opensips
      DRAGONFLY_URL: redis://dragonfly:6379
      CLICKHOUSE_URL: http://clickhouse:8123
      ACM_ENGINE_URL: http://acm-engine:8080
      JWT_SECRET: ${JWT_SECRET:-change_me_in_production}
    depends_on:
      - acm-engine
      - yugabyte
    networks:
      - acm-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  # ==========================================================================
  # MONITORING STACK
  # ==========================================================================
  
  # Prometheus
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9091:9090"
    volumes:
      - ../../monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ../../monitoring/prometheus/alerts:/etc/prometheus/alerts:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=15d'
    networks:
      - acm-network

  # Grafana
  grafana:
    image: grafana/grafana:10.2.0
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-acm_grafana_2026}
      GF_INSTALL_PLUGINS: grafana-clickhouse-datasource
    volumes:
      - grafana-data:/var/lib/grafana
      - ../../monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ../../monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
      - clickhouse
    networks:
      - acm-network

  # Homer SIP Capture
  homer:
    image: sipcapture/homer-app:latest
    container_name: homer
    restart: unless-stopped
    ports:
      - "9080:9080"
    environment:
      DB_HOST: homer-postgres
      DB_USER: homer
      DB_PASS: ${HOMER_DB_PASSWORD:-homer_secure_2026}
    depends_on:
      - homer-postgres
    networks:
      - acm-network

  homer-postgres:
    image: postgres:15
    container_name: homer-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: homer
      POSTGRES_PASSWORD: ${HOMER_DB_PASSWORD:-homer_secure_2026}
      POSTGRES_DB: homer
    volumes:
      - homer-pg-data:/var/lib/postgresql/data
    networks:
      - acm-network

  heplify-server:
    image: sipcapture/heplify-server:latest
    container_name: heplify-server
    restart: unless-stopped
    ports:
      - "9060:9060/udp"
    environment:
      HEPLIFYSERVER_HEPADDR: "0.0.0.0:9060"
      HEPLIFYSERVER_DBDRIVER: postgres
      HEPLIFYSERVER_DBADDR: homer-postgres:5432
      HEPLIFYSERVER_DBUSER: homer
      HEPLIFYSERVER_DBPASS: ${HOMER_DB_PASSWORD:-homer_secure_2026}
      HEPLIFYSERVER_DBNAME: homer
    depends_on:
      - homer-postgres
    networks:
      - acm-network

  # ==========================================================================
  # NCC COMPLIANCE SERVICES
  # ==========================================================================
  
  # Daily SFTP Uploader (runs as cron)
  ncc-sftp-uploader:
    build:
      context: ../../ncc-compliance/sftp-uploader
      dockerfile: Dockerfile
    image: acm/ncc-sftp:2.0
    container_name: ncc-sftp-uploader
    restart: unless-stopped
    environment:
      NCC_SFTP_HOST: ${NCC_SFTP_HOST:-sftp.ncc.gov.ng}
      NCC_SFTP_PORT: 22
      NCC_SFTP_USER: ${NCC_SFTP_USER:-}
      NCC_ICL_LICENSE: ${NCC_ICL_LICENSE:-}
      CLICKHOUSE_URL: http://clickhouse:8123
    volumes:
      - ncc-keys:/etc/acm/keys:ro
      - ncc-reports:/var/acm/reports
    depends_on:
      - clickhouse
    networks:
      - acm-network
    # Runs daily at 01:00 AM WAT
    entrypoint: ["crond", "-f"]

# ==========================================================================
# NETWORKS
# ==========================================================================
networks:
  acm-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.30.0.0/16

# ==========================================================================
# VOLUMES
# ==========================================================================
volumes:
  dragonfly-data:
    driver: local
  yugabyte-data:
    driver: local
  clickhouse-data:
    driver: local
  clickhouse-logs:
    driver: local
  questdb-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  homer-pg-data:
    driver: local
  opensips-tls:
    driver: local
  ncc-keys:
    driver: local
  ncc-reports:
    driver: local
